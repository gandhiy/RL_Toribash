import gym
import sys
import pickle
import numpy as np
import gym_Toribash


from os.path import join
from hmmlearn import hmm
from utils.tools import load_csv
from sklearn.cluster import KMeans
from stable_baselines import PPO1
from sklearn.decomposition import PCA 


from pdb import set_trace



class Stochastic_Similarity:
    def __init__(self, model, configs):
        self.model = model
        self.configs = configs
        self.turns = 100



        # load up the expert trajectories
        df1 = load_csv(join(sys.path[0], "../data/player1_state_info.csv"))
        df2 = load_csv(join(sys.path[0], "../data/player2_state_info.csv"))
        p1 = df1.drop(columns=['Unnamed: 0', 'match', 'turn'])
        p2 = df2.drop(columns=['Unnamed: 0', 'match', 'turn']) 
        df1['match_length'] = 1
        func = {'match_length': lambda l: sum(l)}
        gb = df1.groupby(by='match').agg(func)
        
        self.lengths = list(gb.match_length)
        self.expert_idx = self._lengths_to_idx(self.lengths)
        self.experts = np.concatenate((p1, p2), axis=1)

        # these parameters gave some decent results
        self.pca_dim = 30
        self.hidden_states = 12
        self.clusters = 10
        self.n_iter = 25
        self.verbose = False

        self.expert_codebook_states = None
        self.expert_hmm = None

    @property
    def PCA_Dimension(self):
        return self.pca_dim
    
    @PCA_Dimension.setter
    def PCA_Dimension(self, dim):
        self.pca_dim = dim

    @property
    def number_hidden_states(self):
        return self.hidden_states

    @number_hidden_states.setter
    def number_hidden_states(self, states):
        self.hidden_states = states
    
    @property
    def KMeans_Clusters(self):
        return self.clusters

    @KMeans_Clusters.setter
    def KMeans_Clusters(self, clusters):
        self.clusters = clusters
    
    @property
    def number_iterations(self):
        return self.n_iter
    
    @number_iterations.setter
    def number_iterations(self, iters):
        self.n_iter = iters

    @property
    def Verbose(self):
        return self.verbose

    @Verbose.setter
    def Verbose(self, verbose):
        self.verbose = verbose


    def create_discrete_codebook(self, obs, dim, clusters):
        """"
        PARAMETERS:
        obs (np.ndarray (N, 298)): flattened observation array from running a model 
                                    or from the experts
        dim (int): pca dimension
        clusters (int): Kmeans clusters

        RETURNS:
        codebook_states (np.ndarray (N, 1)): returns the states after being reduced and discretized
        """

        pca = PCA(dim)
        kmeans = KMeans(clusters)
        reduced_obs = pca.fit_transform(obs)
        return kmeans.fit_predict(reduced_obs).reshape((-1,1))
        
        #### Per Row Clustering ####
        # per_row_clusters = []
        # codebook_states = []
        # for j in range(len(reduced_obs[0])):
        #     cluster = KMeans(clusters)
        #     cluster.fit(reduced_obs[:,j].reshape((-1,1)))
        #     per_row_clusters.append(cluster)
        #     codebook_states.append(cluster.predict(reduced_obs[:,j].reshape(-1,1)))
        
        # return np.array(codebook_states).T


    def run_model(self, K, model=None, configs=None):
        if(model is None):
            model = self.model
        if(configs is None):
            configs = self.configs

        with open(configs, 'rb') as f:
            env_dict = pickle.load(f)
        
        env = gym.make(env_dict['env_name'])
        env.settings.set("matchframes", self.turns*10)

        env.init(**env_dict)
        
        with open(model, 'rb') as f:
            h = PPO1.load(f, env=env)
        obs = env.reset()

        generated_trajectories = []
        for episode in range(K):
            tau = []
            for turn in range(self.turns):
                action, _ = h.predict(obs)
                obs, _, done, _ = env.step(action)
                tau.append(obs)
                if(done):
                    env.reset()
            generated_trajectories.append(tau)

        env.close()
        return np.array(generated_trajectories).astype(np.float)


    def _lengths_to_idx(self, lengths):
        idx = [0]
        for l in lengths:
            idx.append(idx[-1] + l)
        return np.array(idx[1:-1]).astype(np.int)

    def _similarity(self, K, l):
        """ 
        calculate the similarity between current model and experts
        over K samples generated by the model
        """


        # # # # create both codebooks_states # # # # 

        # experts
        if(self.expert_codebook_states is None):
            self.expert_codebook_states = self.create_discrete_codebook(
                self.experts, 
                self.pca_dim, 
                self.clusters)
            self.expert_observations = np.split(self.expert_codebook_states, self.expert_idx)

        # model
        model_obs = self.run_model(K, self.model, self.configs) # probably the most time consuming part
        lengths = self.turns*np.ones(shape=K).astype(np.int)
        model_idx = self._lengths_to_idx(lengths)
        model_codebook_states = self.create_discrete_codebook(
            np.concatenate([eps for eps in model_obs]),
            self.pca_dim, 
            self.clusters
        )
        model_observations = np.split(model_codebook_states, model_idx)

        
        # # # # create both hidden markov models # # # # 
        
        # experts
        if(self.expert_hmm is None):
            self.expert_hmm = hmm.GaussianHMM(
                self.hidden_states, 
                n_iter = self.n_iter, 
                verbose = self.verbose).fit(self.expert_codebook_states, self.lengths)

        # model        
        model_hmm = hmm.GaussianHMM(
            self.hidden_states, 
            n_iter = self.n_iter,
            verbose = self.verbose
        ).fit(model_codebook_states, lengths)


        # # # # run over sampels # # # # 
        expert_samples_idx = np.random.choice(np.arange(len(self.lengths)), size=(l,), replace=True)
        model_samples_idx = np.random.choice(np.arange(K), size=(l,), replace=True)
        
        sim_score = []       
        for i,j in list(zip(expert_samples_idx, model_samples_idx)):
            expert_obs = self.expert_observations[i]
            model_obs = model_observations[j]
            P11 = 10**(self.expert_hmm.score(expert_obs)/len(expert_obs))
            P12 = 10**(self.expert_hmm.score(model_obs)/len(model_obs))
            P21 = 10**(model_hmm.score(expert_obs)/len(expert_obs))
            P22 = 10**(model_hmm.score(model_obs)/len(model_obs))

            sim_score.append(np.sqrt((P12*P21)/(P11*P22)))

        return sim_score


    def similarity_score(self, K, l):
        """
         Get the similarity between the model and the expert

         K (int): 
            The number of runs to generate. Used to calculate the PCA reduction
            and Kmeans clustering
        l (int):
            The number of samples to average the score over.

        Return (float): the similarity score average over l samples
        """
        scores = self._similarity(K, l)
        return sum(scores)/len(scores)







        



    


    






